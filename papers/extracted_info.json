{
  "2201.11903_chain_of_thought.pdf": {
    "text_preview": "Chain-of-Thought Prompting Elicits Reasoning\nin Large Language Models\nJasonWei XuezhiWang DaleSchuurmans MaartenBosma\nBrianIchter FeiXia EdH.Chi QuocV.Le DennyZhou\nGoogleResearch,BrainTeam\n{jasonwei,dennyzhou}@google.com\nAbstract\nWeexplorehowgeneratingachainofthought\u2014aseriesofintermediatereasoning\nsteps\u2014significantly improves the ability of large language models to perform\ncomplexreasoning. Inparticular,weshowhowsuchreasoningabilitiesemerge\nnaturallyinsufficientlylargelanguagemodelsviaasimplemethodcalledchain-of-\nthoughtprompting,whereafewchainofthoughtdemonstrationsareprovidedas\nexemplarsinprompting.\nExperimentsonthreelargelanguagemodelsshowthatchain-of-thoughtprompting\nimproves performance on a range of arithmetic, commonsense, and symbolic\nreasoningtasks. Theempiricalgainscanbestriking. Forinstance,promptinga\nPaLM540Bwithjusteightchain-of-thoughtexemplarsachievesstate-of-the-art\naccuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetunedGPT-3withaverifier.\nStandard Prompting Chain-of-Thought Prompting\nModel Input Model Input\nQ: Roger has 5 tennis balls. He buys 2 more cans of Q: Roger has 5 tennis balls. He buys 2 more cans of\ntennis balls. Each can has 3 tennis balls. How many tennis balls. Each can has 3 tennis balls. How many\ntennis balls does he have now? tennis balls does he have now?\nA: The answer is 11. A: Roger started with 5 balls. 2 cans of 3 tennis balls\neach is 6 tennis balls. 5 + 6 = 11. The answer is 11.\nQ: The cafeteria had 23 apples. If they used 20 to\nmake lunch and bought 6 more, how many apples Q: The cafeteria had 23 apples. If they used 20 to\ndo they have? make lunch and bought 6 more, how many apples\ndo they have?\nModel Output Model Output\nA: The cafeteria had 23 apples originally. They used\nA: The answer is 27.\n20 to make lunch. So they had 23 - 20 = 3. They\nbought 6 more apples, so they have 3 + 6 = 9. The\nanswer is 9.\nFigure1: Chain-of-thoughtpromptingenableslargelanguagemodelstotacklecomplexarithmetic,\ncommonsense,an",
    "full_length": 9789
  },
  "2402.10200_cot_without_prompting.pdf": {
    "text_preview": "Chain-of-Thought Reasoning without Prompting\nXuezhiWang1 andDennyZhou1\n1GoogleDeepMind,1{xuezhiw,dennyzhou}@google.com\nIn enhancing the reasoning capabilities of large language models (LLMs), prior research primarily\nfocusesonspecificpromptingtechniquessuchasfew-shotorzero-shotchain-of-thought(CoT)prompting.\nThesemethods,whileeffective,ofteninvolvemanuallyintensivepromptengineering. Ourstudytakes\na novel approach by asking: Can LLMs reason effectively without prompting? Our findings reveal\nthat, intriguingly, CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the\ndecoding process. Rather than conventional greedy decoding, we investigate the top-\ud835\udc58 alternative\ntokens, uncovering that CoT paths are frequently inherent in these sequences. This approach not\nonlybypassestheconfoundersofpromptingbutalsoallowsustoassesstheLLMs\u2019intrinsicreasoning\nabilities. Moreover,weobservethatthepresenceofaCoTinthedecodingpathcorrelateswithahigher\nconfidenceinthemodel\u2019sdecodedanswer. Thisconfidencemetriceffectivelydifferentiatesbetween\nCoTandnon-CoTpaths. Extensiveempiricalstudiesonvariousreasoningbenchmarksshowthatthe\nproposed CoT-decoding effectively elicits reasoning capabilities from language models, which were\npreviouslyobscuredbystandardgreedydecoding.\n1. Introduction\nLarge language models (LLMs) have demonstrated remarkable performance on various complicated\nreasoning benchmarks (Anil et al., 2023; Brown et al., 2020; Chowdhery et al., 2023; Gemini, 2023;\nOpenAI,2023;Romera-Paredesetal.,2023). ThesereasoningcapabilitiesofLLMsaretypicallyelicited\nby prompting techniques (Brown et al., 2020), which can be few-shot prompting with intermediate\nstepsaugmenteddemonstrationexemplars(Chenetal.,2023b;Gaoetal.,2022;Nyeetal.,2021;Wei\net al., 2022; Yao et al., 2023; Zhou et al., 2023a), or zero-shot prompting with specific instructions\nwhich ask for showing certain intermediate steps (Kojima et al., 2022; Yasunaga et al., 2023). The\nother prevalent strategy for elicit",
    "full_length": 10215
  },
  "2405.08644_thinking_tokens.pdf": {
    "text_preview": "Thinking Tokens for Language Modeling\nDavid Herel, Tomas Mikolov\nFaculty of Electrical Engineering, Czech Technical University in Prague\nCzech Institute of Informatics, Robotics and Cybernetics, Czech Technical University in Prague\nhereldav@fel.cvut.cz\nAbstract\nHow much is 56 times 37? Language models often make mistakes in these types of\ndifficult calculations. This is usually explained by their inability to perform complex rea-\nsoning. Sincelanguagemodelsrelyonlargetrainingsetsandgreatmemorizationcapabil-\nity, naturally they are not equipped to run complex calculations. However, one can argue\nthat humans also cannot perform this calculation immediately and require a considerable\namountoftimetoconstructthesolution. Inordertoenhancethegeneralizationcapability\noflanguagemodels,andasaparalleltohumanbehavior,weproposetousespecial\u2019think-\ningtokens\u2019whichallowthemodeltoperformmuchmorecalculationswheneveracomplex\nproblem is encountered.\n1 Introduction\nLanguage models based on neural networks have gained a great deal of interest in recent years\n[15, 14]. Their impressive and coherent answers amazed people across many industries. How-\never,ithassoonbeendiscoveredthattheselanguagemodelshaveproblemswithcomplextasks\n[5, 4].\nComplex questions suchas \u2019how much is 56 times 37\u2019 which are computationallyrequiring,\nareproblematicforthelanguagemodeltoprocessorevenanswercorrectly. Onecanarguethat\nhumans also cannot perform this calculation right away and require a considerable amount of\ntime to provide a solution.\nAlthough these reasoning abilities could\nbe improved by employing a large amount of\nsupervision by providing labeled examples to\nthemodel[4], weaimforamuchfasterunsu-\npervised approach.\nTo enhance the generalization capability\noflanguagemodels,weproposetousespecial\n\u2019thinking tokens\u2019 which allow the model to\nFigure1: Illustrationof\u2019thinkingtokens\u2019(marked\nperform much more calculations whenever a\nas <T>) in a sentence which requires a complex\ncomplex problem is encountered. Thi",
    "full_length": 9172
  },
  "2412.13171_compressed_cot.pdf": {
    "text_preview": "Compressed Chain of Thought: Efficient Reasoning through\nDense Representations\nJeffreyCheng1 BenjaminVanDurme1\nAbstract ure1withCoTprompting,whereasitcananswerthesame\nquestionwithoutCoTpromptingin2.81seconds,achieving\nChain-of-thought (CoT) decoding enables lan-\nthesameanswerwithanalmost10xspeedup.\nguagemodelstoimprovereasoningperformance\natthecostofhighgenerationlatencyindecoding. Pastworkhasutilizedwhatwetermcontemplationtokens\nRecentproposalshaveexploredvariantsofcon- asanalternativetoexplicitCoTreasoningtraces(Pfauetal.,\ntemplationtokens,atermweintroducethatrefers 2024;Goyaletal.,2024). Theseareadditionaltokensused\ntospecialtokensusedduringinferencetoallow tointroduceonlinememory,allowingforadditionalcom-\nforextracomputation. Priorworkhasconsidered putationsduringinference. Insteadofgeneratingareason-\nfixed-lengthsequencesdrawnfromadiscreteset ing chain entirely of explicit language tokens, the model\nofembeddingsascontemplationtokens. Herewe conditionsonashortersequenceofcontemplationtokens\nproposeCompressedChain-of-Thought(CCoT), (Section2). Contemplationtokenscaneitherbecontentful,\na framework to generate contentful and contin- groundedinsemanticallymeaningfultext,ornoncontentful.\nuouscontemplationtokensofvariablesequence Therearemanylinesofpriorworkinvolvingnoncontentful\nlength. Thegeneratedcontemplationtokensare contemplationtokensdrawnfromasetofdiscretetokens;\ncompressedrepresentationsofexplicitreasoning thispaperintroducescontentfulcontemplationtokensthat\nchains,andourmethodcanbeappliedtooff-the- representreasoningchainsperformedincontinuousspace.\nshelfdecoderlanguagemodels. Throughexperi-\nOur framework, called Compressed Chain of Thought\nments,weillustratehowCCoTenablesadditional\n(CCoT), generates contemplation tokens which are com-\nreasoningoverdensecontentfulrepresentations\npressedrepresentationsoflanguage-basedreasoningchains.\ntoachievecorrespondingimprovementsinaccu-\nThesecontemplationtokensaretrainedthroughteacherforc-\nracy. Moreover,thereasoningimpr",
    "full_length": 11709
  },
  "2506.08343_removing_thinking_tokens.pdf": {
    "text_preview": "5202\nnuJ\n81\n]LC.sc[\n2v34380.6052:viXra\nWait, We Don\u2019t Need to \u201cWait\u201d!\nRemoving Thinking Tokens Improves Reasoning Efficiency\nChenlongWang,YuanningFeng,DongpingChen,ZhaoyangChu1,\nRanjayKrishna\u20202,TianyiZhou\u2020\n1UniversityCollegeLondon,2UniversityofWashington\nAbstract Zhou et al., 2025) and videos (Feng et al., 2025;\nTeam,2024;Teametal.,2025).\nRecent advances in large reasoning models\nDespitetheeffectivenessoflongCoTreasoning\nhaveenabledcomplex,step-by-stepreasoning\nwithself-reflection,theoverthinkingproblemhas\nbutoftenintroducesignificantoverthinking,re-\nemerged(Chenetal.,2024a;Cuadronetal.,2025;\nsultinginverboseandredundantoutputsthat\nChenetal.,2024b;Wuetal.,2025;Suietal.,2025).\nhinder efficiency. In this study, we examine\nwhetherexplicitself-reflection,signaledbyto- Itischaracterizedbyexcessivelyverbosereason-\nkenssuchas\u201cWait\u201dand\u201cHmm\u201d,isnecessary ingandredundantthoughtsteps, oftenextending\nforadvancedreasoning. WeproposeNOWAIT, over thousands of tokens, resulting in significant\na simple yet effective approach that disables\ncomputationaloverheadandhighreasoninglatency.\nexplicitself-reflectionbysuppressingtheseto-\nSuchinefficiencieshinderthepracticaldeployment\nkensduringinference. Extensiveexperiments\nofR1-stylereasoningmodelsinapplicationswith\nontenbenchmarksacrosstextual,visual,and\nlimitedcomputationalresources.\nvideo reasoning tasksshow that NOWAIT re-\nduceschain-of-thoughttrajectorylengthbyup Although numerous efforts have been devoted\nto 27%\u201351% in five R1-style model series, to efficient reasoning, many existing approaches\nwithoutcompromisingmodelutility. NOWAIT requireadditionaltraining,eitherthroughreinforce-\nthusoffersaplug-and-playsolutionforefficient mentlearning(RL)withlength-basedrewards(Ag-\nandutility-preservingmultimodalreasoning.\ngarwalandWelleck,2025;Liaoetal.,2025;Luo\netal.,2025)orfine-tuningonvariable-lengthCoT\n1 Introduction\ntrajectories (Ma et al., 2025b; Munkhbat et al.,\nRecent advancements in large reasoning models 2025). On the other hand, severa",
    "full_length": 11926
  },
  "2506.08410_llm_metacognition.pdf": {
    "text_preview": "Large Language Models Have Intrinsic Meta-Cognition,\nbut Need a Good Lens\nZiyangMa1*,QingyueYuan2*,ZhenglinWang1,DeyuZhou1\u2020\n1 SchoolofComputerScienceandEngineering,KeyLaboratoryofComputerNetwork\nandInformationIntegration,MinistryofEducation,SoutheastUniversity,China\n2 DepartmentofNeurosurgery,ShanghaiTenthPeople\u2019sHospital,Schoolof\nClinicalMedicineofNanjingMedicalUniversity,China\n{mazy, zhenglin, d.zhou}@seu.edu.cn yuanqy007@stu.njmu.edu.cn\nAbstract\nI S n t t a e t r e n s al R in e g a S so te n p - Answer (b)Self-Evaluation\nPreviousresearchhasprimarilyfocusedonthe 1 L & LM 2 P R M r e o o w d c e a e l r s d s M C Le o e n g ta s n - ition\ncognitiveerrordetectioncapabilitiesofLarge Step Forward Work Flow Dependency\n\u00b7 Perplexity\nLanguage Models (LLMs), often prompting \u00b7 Entropy\nQuestion \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nthemtoanalyzemistakesinreasoningchains.\nVerifier\nHowever,fewstudieshaveexaminedthemeta-\nAnswer: correct correct\ncognitive abilities of LLMs (e.g., their self-\nawareness of step errors), which are crucial (a)Error Detection (c)Our Study\nfor their reliability. While studies on LLM\nself-evaluation present some measures, such\nasperplexity,whichcanreflecttheanswercor-\nrectness and be viewed as the lens of meta-\nHuman\ncognition, they lack step-level analysis and Beings\nadaptation. Thispaperstudiestheevaluationof\nLLMmeta-cognitionusingthecurrentlenses Step 1: correct correct Step 1: correct correct correct\nStep 2: correct correct Step 2: correct correct correct\nandhowtoimprovetheselenses. Specifically, Step 3: wrong correct Step 3: correct wrong wrong\nwe propose AutoMeco, an Automated Meta-\ncognition Evaluation framework for bench- Figure1: Inreasoningtasks,errordetection(a)focuses\nmarking the existing lenses. Furthermore, a onLLMs\u2019cognitiveabilitytoanalyzeerrorsinreason-\ntraining-freeMarkovianIntrinsicRewardAd- ing steps. Self-evaluation (b) utilizes measures such\njustmentstrategy,MIRA,isproposedtoboost asentropyaslensestoreflectself-awarenessofanswer\ncurrentmeta-cognitionlenses.Exp",
    "full_length": 12295
  },
  "2506.23840_do_thinking_tokens_help.pdf": {
    "text_preview": "5202\nnuJ\n03\n]LC.sc[\n1v04832.6052:viXra\nDo Thinking Tokens Help or Trap?\nTowards More Efficient Large Reasoning Model\nBowen Ding1,4,\u2217, Yuhan Chen2,\u2217, Futing Wang1,4, Lingfeng Ming3, and Tao Lin4,5,\u2020\n1 Zhejiang University 2 Boston University 3 ByteDance\n4 School of Engineering, Westlake University\n5 Research Center for Industries of the Future, Westlake University\n4{dingbowen, wangfuting, lintao}@westlake.edu.cn\n2erv1n@bu.edu, 3minglingfeng@bytedance.com\nAbstract\nLarge Reasoning Models (LRMs) excel at\nsolving complex problems but face an over-\nthinking dilemma. When handling simple\ntasks,theyoftenproduceverboseresponses\noverloaded with thinking tokens (e.g., wait,\nhowever). These tokens trigger unneces-\nsary high-level reasoning behaviors like re-\nflection and backtracking, reducing effi-\nciency. Inthiswork,ourpilotstudyreveals\nthat these thinking-token-induced behav-\nFigure 1: The Illustration of Trapped v.s. Effi-\niors are not essential for effective problem- cient Reasoning. AnexamplefromMATH500where\nsolving and may even hinder correct rea- correct inference gets stuck in redundant verification\nsoning within constrained token budgets. loops, failing to produce a final answer within token\nWe identify this phenomenon as the think- limits.\ning trap. To mitigate this issue, we pro-\npose Dual Policy Preference Optimization\n(DuP-PO),anovelalgorithmfeaturing: (1) conventional Large Language Models (LLMs),\nArolloutsamplingstrategythatguarantees LRMs consistently generate discourse mark-\nbalanced exposure to responses with and ers such as \u201cwait, hmm, however\u201d within their\nwithout thinking tokens; (2) A fine-grained\nreasoning processes, which we term thinking\nadvantagecontroltechniquetodynamically\ntokens (elaborated in Section 5.3.1). These\nregulate the prediction of target tokens; (3)\ntokens, in turn, activate advanced cognitive\nA policy shaping method ensuring stable\nbehaviours such as reflection, back-tracking,\ngradientcontributionsfromthinkingtokens.\nExperimental results",
    "full_length": 11333
  },
  "2510.03259_meta_awareness.pdf": {
    "text_preview": "Preprint\nMETA-AWARENESS ENHANCES REASONING MODELS:\nSELF-ALIGNMENT REINFORCEMENT LEARNING\nYoonjeonKim1\u2217 DoohyukJang1\u2217 EunhoYang1,2\n1KAIST 2AITRICS\nABSTRACT\nRecentstudiesonreasoningmodelsexplorethemeta-awarenessoflanguagemod-\nels, the ability to know \u2018how to think\u2019 by itself. We argue that large reasoning\nmodels lack this meta-awareness property by proving severe misalignment be-\ntweentruerolloutsandpredictedmetainformation. Wepositthataligningmeta-\nprediction with true rollouts will lead to significant performance gains. To ver-\nifythishypothesis,wedesignatrainingpipelinethatboostsMeta-Awarenessvia\nSelf-Alignment(MASA),andprovethatenhancedmeta-awarenessdirectlytrans-\nlatestoimprovedaccuracy. Unlikeexistingmeta-cognitivereasoningmodels,our\nmethoddoesnotrequireexternaltrainingsourcesbutleveragesself-generatedsig-\nnalstotrainmeta-awareness. Moreover,ourmethodenablesefficienttrainingby\ni) filtering out zero-variance prompts that are either trivial or unsolvable and ii)\ncuttingofflengthyrolloutswhentheyareunlikelytoleadtocorrectanswers. The\nresults are inspiring: our strategy yields significant improvements in both accu-\nracy and training efficiency on in-domain tasks and shows strong generalization\ntoout-of-domainbenchmarks. Morespecifically,ourmethodcanspeedupGRPO\ntrainingbyover1.28 toreachthesameperformance,andachievea19.3%gain\n\u00d7\nin accuracy on AIME25, and a 6.2% average gain over six mathematics bench-\nmarks. Training with meta-cognitive guidance enhances out-of-domain general-\nization,givinga3.87%boostonGPQA-Diamondanda2.08%overallaccuracy\ngainacross13benchmarksspanninglogical,scientific,andcodingdomains. The\ncodeisavailableathttps://github.com/akatigre/MASA-RL.\nytluciffiD\nlautcA\nhtgneL\nlautcA\nPredicted Difficulty Predicted Length\n(a)PoorMeta-AwarenessofGRPOModel\nytluciffiD\nlautcA\nhtgneL\nlautcA\nPredicted Difficulty Predicted Length\n(b)EnhancedMeta-AwarenessofMASA\n<latexit sha1_base64=\"fdKy1KcxQeLoaVFWHF7xEOOUyho=\">AAACCnicbVDJSgNBEO1xN25Rj15ag6CXMCNuR1EPHhWzCEkINZ2KN",
    "full_length": 18104
  }
}